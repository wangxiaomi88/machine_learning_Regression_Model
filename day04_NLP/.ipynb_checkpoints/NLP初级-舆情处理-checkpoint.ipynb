{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb26029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk.tokenize as tk\n",
    "\n",
    "\n",
    "doc =\"Are you curious about tokenization? \"\\\n",
    "\"Let's see how it works! \"\\\n",
    "\"We need to analyze a couple of sentences \"\\\n",
    "\"with punctuations to see it in action.\"\n",
    "\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cee28c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are you curious about tokenization?\n",
      "2 : Let's see how it works!\n",
      "3 : We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "#分句子\n",
    "sents = tk.sent_tokenize(doc)\n",
    "for i in range(len(sents)):\n",
    "    print(i+1,\":\",sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63b8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are\n",
      "2 : you\n",
      "3 : curious\n",
      "4 : about\n",
      "5 : tokenization\n",
      "6 : ?\n",
      "7 : Let\n",
      "8 : 's\n",
      "9 : see\n",
      "10 : how\n",
      "11 : it\n",
      "12 : works\n",
      "13 : !\n",
      "14 : We\n",
      "15 : need\n",
      "16 : to\n",
      "17 : analyze\n",
      "18 : a\n",
      "19 : couple\n",
      "20 : of\n",
      "21 : sentences\n",
      "22 : with\n",
      "23 : punctuations\n",
      "24 : to\n",
      "25 : see\n",
      "26 : it\n",
      "27 : in\n",
      "28 : action\n",
      "29 : .\n"
     ]
    }
   ],
   "source": [
    "#分单词\n",
    "words = tk.word_tokenize(doc)\n",
    "for i in range(len(words)):\n",
    "    print(i+1,\":\",words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53201773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are\n",
      "2 : you\n",
      "3 : curious\n",
      "4 : about\n",
      "5 : tokenization\n",
      "6 : ?\n",
      "7 : Let\n",
      "8 : '\n",
      "9 : s\n",
      "10 : see\n",
      "11 : how\n",
      "12 : it\n",
      "13 : works\n",
      "14 : !\n",
      "15 : We\n",
      "16 : need\n",
      "17 : to\n",
      "18 : analyze\n",
      "19 : a\n",
      "20 : couple\n",
      "21 : of\n",
      "22 : sentences\n",
      "23 : with\n",
      "24 : punctuations\n",
      "25 : to\n",
      "26 : see\n",
      "27 : it\n",
      "28 : in\n",
      "29 : action\n",
      "30 : .\n"
     ]
    }
   ],
   "source": [
    "# 另一种分单词方法\n",
    "tokenizer = tk.WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(doc)\n",
    "for i in range(len(words)):\n",
    "    print(i+1,\":\",words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ebf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916b4692",
   "metadata": {},
   "source": [
    "# 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "704f6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9be9b6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1]]\n",
      "['bad', 'environment', 'good', 'hotel', 'in', 'is', 'of', 'smells', 'the', 'this', 'toilet', 'very']\n"
     ]
    }
   ],
   "source": [
    "sents = [\"This hotel is very bad.\",\n",
    "\"The toilet in this hotel smells bad.\",\n",
    "\"The environment of this hotel is very good.\"]\n",
    "\n",
    "\n",
    "cv = ft.CountVectorizer()\n",
    "bow = cv.fit_transform(sents).toarray()\n",
    "print(bow)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136564de",
   "metadata": {},
   "source": [
    "# 词频逆文档频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "096ed65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.488 0.    0.    0.379 0.    0.488 0.    0.    0.    0.379 0.    0.488]\n",
      " [0.345 0.    0.    0.268 0.454 0.    0.    0.454 0.345 0.268 0.454 0.   ]\n",
      " [0.    0.429 0.429 0.253 0.    0.326 0.429 0.    0.326 0.253 0.    0.326]]\n",
      "['bad', 'environment', 'good', 'hotel', 'in', 'is', 'of', 'smells', 'the', 'this', 'toilet', 'very']\n"
     ]
    }
   ],
   "source": [
    "tt = ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow).toarray()\n",
    "\n",
    "print(np.round(tfidf,3))\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc67a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f675b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bef8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a248ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"An article.txt\",\"rb\") as f:\n",
    "#     data = f.read()\n",
    "#     data = data.decode()\n",
    "\n",
    "#     sents = tk.sent_tokenize(data)\n",
    "#     for i in range(len(sents)):\n",
    "#         print(i + 1, \":\", sents[i])\n",
    "    \n",
    "#     cv = ft.CountVectorizer()\n",
    "#     bow = cv.fit_transform(sents).toarray()\n",
    "#     print(bow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7346aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69c3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec090998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
